# References

Following are the references to all the deep learning models, used in this project, along with brief descriptions of how each one was used.

## Deep Learning Models

### [YOLOv11x](https://github.com/bhaskrr/traffic-sign-detection-using-yolov11)
- **Authors**: Ultralytics
- **Year**: 2024
- **Citation**: Ultralytics, "YOLOv11: A state-of-the-art object detection model," GitHub repository, 2024. [Online]. Available: https://github.com/bhaskrr/traffic-sign-detection-using-yolov11
- **Usage**: Used for detecting vehicles and stop signs with high accuracy in various environmental conditions.

### [Detic](https://github.com/CityIsBetter/Traffic-Light-Detection-YOLOv8)
- **Authors**: X. Zhou, V. Koltun, and P. Kr채henb체hl
- **Year**: 2022
- **Citation**: X. Zhou, V. Koltun, and P. Kr채henb체hl, "Detic: Detecting Twenty-thousand Classes Using Image-level Supervision," in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.*, 2022, pp. 10883-10893.
- **Usage**: Used for detecting traffic light and classifying signal states as (red, yellow, green).

### [YOLOv5s](https://github.com/ultralytics/yolov5)
- **Authors**: Ultralytics
- **Year**: 2022
- **Citation**: Ultralytics, "YOLOv5: State-of-the-art object detection at the speed required for real-time processing," GitHub repository, 2022. [Online]. Available: https://github.com/ultralytics/yolov5
- **Usage**: Used for pedestrian detection in various scenarios.

### [Mask RCNN for Lane Detection](https://debuggercafe.com/lane-detection-using-mask-rcnn/)
- **Authors**: Debugger Cafe
- **Year**: 2023
- **Citation**: Debugger Cafe, "Lane Detection using Mask RCNN," 2023. [Online]. Available: https://debuggercafe.com/lane-detection-using-mask-rcnn/
- **Usage**: Used for lane detection and segmentation using instance segmentation approach. This model identifies six different types of lanes (divider-line, dotted-line, double-line, random-line, road-sign-line, solid-line).

### [Depth Anything v2](https://github.com/LiheYoung/Depth-Anything)
- **Authors**: Y. Li, Z. Yang, Y. Wang, S. Cai, T. Chen, J. Liu, M. Yang, L. Van Gool, and D. Dai
- **Year**: 2024
- **Citation**: Y. Li et al., "Depth Anything v2: Stronger Rural and Urban Monocular Depth Estimation," arXiv preprint arXiv:2404.04048, 2024.
- **Usage**: Used for generating high-quality monocular depth maps from video frames, enabling depth estimation of vehicles, pedestrians, traffic signs, and other scene elements for accurate 3D positioning.

---
